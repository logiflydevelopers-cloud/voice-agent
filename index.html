<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Voice AI Call</title>

  <style>
    body {
      background: #0f172a;
      color: #fff;
      font-family: Arial;
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100vh;
    }
    .box {
      background: #020617;
      width: 360px;
      padding: 25px;
      border-radius: 14px;
    }
    button {
      width: 100%;
      padding: 16px;
      font-size: 18px;
      border: none;
      border-radius: 40px;
      cursor: pointer;
    }
    .start { background: #22c55e; }
    .stop { background: #ef4444; color: #fff; }
    .log {
      margin-top: 15px;
      font-size: 14px;
      max-height: 200px;
      overflow: auto;
    }
    .user { color: #38bdf8; }
    .ai { color: #a7f3d0; }
    .status { opacity: .8; margin-top: 8px; }
  </style>
</head>

<body>
<div class="box">
  <h3>ðŸŽ§ Voice AI Call</h3>
  <button id="btn" class="start">ðŸŽ¤ Speak</button>
  <div class="status" id="status">Idle</div>
  <div class="log" id="log"></div>
</div>

<script>
/* ================= CONFIG ================= */
const WEBHOOK_URL = "https://newsworld.app.n8n.cloud/webhook/voice-agent";
const SILENCE_THRESHOLD = 0.02;
const SILENCE_TIME = 1200;

/* ================= STATE ================= */
let callActive = false;
let audioCtx, analyser, micStream, recorder;
let chunks = [];
let silenceStart = null;
let currentAudio = null;
let micStopped = true;
let waitingForUser = false;

/* ================= UI ================= */
const btn = document.getElementById("btn");
const status = document.getElementById("status");
const log = document.getElementById("log");

function addLog(text, cls) {
  const div = document.createElement("div");
  div.className = cls;
  div.innerText = text;
  log.appendChild(div);
  log.scrollTop = log.scrollHeight;
}

/* ================= MIC START ================= */
async function startMic() {
  if (!callActive || !waitingForUser) return;

  waitingForUser = false;
  micStopped = false;

  micStream = await navigator.mediaDevices.getUserMedia({ audio: true });

  audioCtx = new AudioContext({ latencyHint: "interactive" });
  const src = audioCtx.createMediaStreamSource(micStream);
  analyser = audioCtx.createAnalyser();
  analyser.fftSize = 2048;
  src.connect(analyser);

  recorder = new MediaRecorder(micStream);
  chunks = [];

  recorder.ondataavailable = e => chunks.push(e.data);
  recorder.onstop = sendToWorkflow;

  recorder.start();
  silenceStart = null;
  status.innerText = "Listening...";
  listenForSilence();
}

/* ================= MIC STOP ================= */
function stopMicCompletely() {
  if (micStopped) return;
  micStopped = true;

  if (recorder && recorder.state !== "inactive") recorder.stop();
  if (micStream) micStream.getTracks().forEach(t => t.stop());
  if (audioCtx) audioCtx.close();

  micStream = null;
  audioCtx = null;
  analyser = null;
}

/* ================= SILENCE DETECTION ================= */
function listenForSilence() {
  const data = new Uint8Array(analyser.fftSize);

  function loop() {
    if (!callActive || !analyser) return;

    analyser.getByteTimeDomainData(data);
    let sum = 0;
    for (let i = 0; i < data.length; i++) {
      const v = (data[i] - 128) / 128;
      sum += v * v;
    }
    const volume = Math.sqrt(sum / data.length);

    if (volume < SILENCE_THRESHOLD) {
      if (!silenceStart) silenceStart = Date.now();
      if (Date.now() - silenceStart > SILENCE_TIME) {
        stopMicCompletely();
        return;
      }
    } else {
      silenceStart = null;
    }
    requestAnimationFrame(loop);
  }
  loop();
}

/* ================= SEND TO N8N ================= */
async function sendToWorkflow() {
  if (!callActive || chunks.length === 0) return;

  status.innerText = "Thinking...";
  addLog("ðŸ§‘ User spoke", "user");

  const blob = new Blob(chunks, { type: "audio/webm" });
  chunks = [];

  const fd = new FormData();
  fd.append("audio", blob, "voice.webm");

  const res = await fetch(WEBHOOK_URL, {
    method: "POST",
    body: fd
  });

  if (!res.ok) {
    status.innerText = "Server error";
    return;
  }

  const buffer = await res.arrayBuffer();
  const audioBlob = new Blob([buffer]);
  currentAudio = new Audio(URL.createObjectURL(audioBlob));

  status.innerText = "Replying...";
  addLog("ðŸ¤– AI replied", "ai");

  await currentAudio.play();

  currentAudio.onended = () => {
    if (!callActive) return;
    status.innerText = "Waiting for user...";
    waitingForUser = true;   // âœ… mic will start ONLY when user speaks again
  };
}

/* ================= BUTTON ================= */
btn.onclick = async () => {
  if (!callActive) {
    callActive = true;
    btn.innerText = "â¹ Stop";
    btn.className = "stop";
    addLog("ðŸ“ž Call started", "user");
    status.innerText = "Waiting for user...";
    waitingForUser = true;   // mic not started yet
  } else {
    callActive = false;
    btn.innerText = "ðŸŽ¤ Speak";
    btn.className = "start";
    status.innerText = "Call ended";
    addLog("âŒ Call ended", "user");

    stopMicCompletely();
    if (currentAudio) currentAudio.pause();
  }
};

/* ================= AUTO VOICE TRIGGER ================= */
navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
  const ctx = new AudioContext();
  const src = ctx.createMediaStreamSource(stream);
  const analyserNode = ctx.createAnalyser();
  analyserNode.fftSize = 2048;
  src.connect(analyserNode);

  const data = new Uint8Array(analyserNode.fftSize);

  function detectVoice() {
    analyserNode.getByteTimeDomainData(data);
    let sum = 0;
    for (let i = 0; i < data.length; i++) {
      const v = (data[i] - 128) / 128;
      sum += v * v;
    }
    const volume = Math.sqrt(sum / data.length);

    if (callActive && waitingForUser && volume > SILENCE_THRESHOLD) {
      startMic();   // ðŸŽ¯ ONLY HERE mic starts
    }
    requestAnimationFrame(detectVoice);
  }
  detectVoice();
});
</script>
</body>
</html>
